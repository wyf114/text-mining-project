{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "46322fb5-5918-4b70-9689-9e0781439ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "daf1e3d1-75ac-4299-8bed-2f413a49f9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim import similarities\n",
    "from gensim import models\n",
    "\n",
    "from wordcloud import WordCloud, ImageColorGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037e74fc-bbcd-43e3-8346-799920cca8d8",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa0e454-e5ff-4585-9c5e-0b2e5d6c0b7f",
   "metadata": {},
   "source": [
    "### EDA at words level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d2ab2ad8-60d7-42cc-912b-2a2ccf6bacb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10002.txt', '10007.txt', '10852.txt', '11.txt', '11870.txt', '120.txt', '12406.txt', '13117.txt', '13610.txt', '13635.txt', '13791.txt', '13888.txt', '13944.txt', '14328.txt', '146.txt', '15478.txt', '15479.txt', '15480.txt', '15489.txt', '16.txt', '16116.txt', '16331.txt', '1656.txt', '16702.txt', '16965.txt', '1727.txt', '17388.txt', '17851.txt', '18183.txt', '18843.txt', '18880.txt', '1951.txt', '19594.txt', '1mart10.txt', '20203.txt', '20467.txt', '20521.txt', '20778.txt', '2130.txt', '22460.txt', '23609.txt', '23680.txt', '24506.txt', '24518.txt', '24708.txt', '2488.txt', '2529.txt', '26377.txt', '26867.txt', '2892.txt', '2rbnh10.txt', '3029.txt', '31529.txt', '31547.txt', '31671.txt', '32032.txt', '345.txt', '34901.txt', '35534.txt', '375.txt', '3800.txt', '38750.txt', '389.txt', '41479.txt', '4339.txt', '4352.txt', '4363.txt', '51233.txt', '514.txt', '5200.txt', '529.txt', '55.txt', '599.txt', '624.txt', '636.txt', '6763.txt', '73.txt', '7370.txt', '7524.txt', '7crmp10.txt', '7jgst10.txt', '84.txt', '8486.txt', '8492.txt', '852.txt', '8crmp10.txt', '8jgst10.txt', '8kngy10.txt', '903.txt', '926.txt', '940.txt', '964.txt', 'alice30.txt', 'analmd10.txt', 'badge10.txt', 'bygdv10.txt', 'cmgrc10.txt', 'dmedu10.txt', 'dracu10.txt', 'dracu11.txt', 'dracu12.txt', 'dracu13.txt', 'ethic10.txt', 'ethic11.txt', 'frank10.txt', 'frank10a.txt', 'frank11.txt', 'frank11a.txt', 'frank12.txt', 'frank12a.txt', 'frank13.txt', 'frank14.txt', 'frank15.txt', 'ggpan10.txt', 'knart10.txt', 'laemc10.txt', 'lkbak10.txt', 'lprss11.txt', 'lwmen10.txt', 'lwmen11.txt', 'lwmen12.txt', 'lwmen13.txt', 'mbng10.txt', 'metam10.txt', 'mohic10.txt', 'nkrnn09.txt', 'nkrnn10.txt', 'nkrnn11.txt', 'nrvcs10.txt', 'owlcr10.txt', 'owlcr11.txt', 'pandp09.txt', 'pandp10.txt', 'pandp11.txt', 'pandp12.txt', 'peter16.txt', 'poeti10.txt', 'pplgy10.txt', 'resof10.txt', 'robots.txt', 'rshft10.txt', 'treas10.txt', 'treas11.txt', 'trgov10.txt', 'utopi10.txt', 'vfair10.txt', 'vfair11.txt', 'vfair12.txt', 'whtco10.txt', 'whtco11.txt', 'whtco12.txt', 'wizoz10.txt', 'wltnt10.txt', 'wltnt11.txt', '~readme.txt']\n",
      "Number of documents in the collection:  155\n",
      "Number of words in each of the books:  [66632, 37417, 73678, 37857, 198465, 89323, 24226, 255229, 172681, 155552, 18649, 85846, 101235, 54981, 84905, 137827, 132013, 148315, 64611, 62472, 54521, 159082, 22172, 153063, 44158, 152936, 97380, 36041, 292997, 48347, 120574, 64772, 69228, 201104, 96298, 80563, 140050, 173369, 53010, 137518, 168191, 19514, 11380, 332516, 37222, 179810, 106865, 142659, 8610, 82300, 133654, 56853, 54735, 17021, 30818, 24001, 196320, 63016, 109614, 8184, 110737, 160131, 29524, 142215, 65529, 49698, 80036, 18054, 233104, 29085, 57119, 50344, 371651, 93443, 116863, 26220, 59152, 69545, 63288, 259317, 57394, 89239, 58252, 91223, 156696, 259319, 57400, 90364, 183512, 184338, 176770, 135180, 36285, 105566, 57338, 79302, 63505, 154758, 190873, 190881, 190881, 191638, 109807, 110020, 87536, 87473, 87537, 87472, 87536, 87472, 87536, 88284, 89287, 27831, 55547, 48843, 91843, 83762, 230910, 230910, 231942, 232307, 130863, 27714, 175943, 432995, 433524, 431847, 64735, 6465, 6462, 17457, 146079, 146825, 146886, 60677, 25496, 20834, 55717, 8, 81198, 87742, 87741, 68690, 51715, 434476, 370071, 370335, 181883, 182662, 182654, 48748, 439215, 440192, 65]\n"
     ]
    }
   ],
   "source": [
    "file_directory = '/Users/wyf/Downloads/is450/text-mining-project/text'\n",
    "\n",
    "filename_pattern = '.+\\.txt'\n",
    "\n",
    "corpus = PlaintextCorpusReader(file_directory, filename_pattern, encoding='latin-1')\n",
    "fids = corpus.fileids()\n",
    "print(fids)\n",
    "print(\"Number of documents in the collection: \", len(fids))\n",
    "\n",
    "docs_len = [len(corpus.words(f)) for f in fids]\n",
    "print(\"Number of words in each of the books: \", docs_len)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e8131433-8bdd-4b01-b9c5-bb1d30209f7f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# number of unique words for each of the books (before data pre-processing)\n",
    "docs = [corpus.words(f) for f in fids]\n",
    "\n",
    "# Number of *unique* words in the books\n",
    "unique_words = list()\n",
    "for book in docs:\n",
    "    fdist_doc = nltk.FreqDist(book)\n",
    "    filtered_word_freq = dict((word, freq) for word, freq in fdist_doc.items() if not word.isdigit())\n",
    "    unique_words.append(len(filtered_word_freq))\n",
    "print(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9ebde7-6a5e-48e3-a553-cab7c68cb85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take quite long, run it only when necessary \n",
    "\n",
    "# data pre-processing, lowercase, stopwords removal and stemming\n",
    "docs_lower = [[w.lower() for w in doc] for doc in docs]\n",
    "docs_alpha = [[w for w in doc if re.search('^[a-z]+$', w)] for doc in docs_lower]\n",
    "\n",
    "stop_list = stopwords.words('english')            \n",
    "docs_stop = [[w for w in doc if w not in stop_list] for doc in docs_alpha]\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "docs_stem = [[stemmer.stem(w) for w in doc] for doc in docs_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ba3019ce-b30b-4a81-b354-f73f1468625a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['project', 'gutenberg', 'hous', 'borderland', 'william', 'hope', 'hodgson', 'ebook', 'use', 'anyon', 'anywher', 'cost', 'almost', 'restrict', 'whatsoev', 'may', 'copi', 'give', 'away', 'use', 'term', 'project', 'gutenberg', 'licens', 'includ', 'ebook', 'onlin', 'www', 'gutenberg', 'org', 'titl', 'hous', 'borderland', 'author', 'william', 'hope', 'hodgson', 'releas', 'date', 'novemb', 'ebook', 'last', 'updat', 'januari', 'languag', 'english', 'charact', 'set', 'encod', 'ascii', 'start', 'project', 'gutenberg', 'ebook', 'hous', 'borderland', 'produc', 'suzann', 'shell', 'sjaani', 'pg', 'distribut', 'proofread', 'hous', 'borderland', 'william', 'hope', 'hodgson', 'manuscript', 'discov', 'messr', 'tonnison', 'berreggnog', 'ruin', 'lie', 'south', 'villag', 'kraighten', 'west', 'ireland', 'set', 'father', 'whose', 'feet', 'tread', 'lost', 'aeon', 'open', 'door', 'listen', 'wind', 'muffl', 'roar', 'glisten', 'tear', 'round', 'moon', 'fanci', 'tread', 'vanish']\n"
     ]
    }
   ],
   "source": [
    "print(docs_stem[0][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "558ae9fa-c270-4eb0-8639-28e681788999",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5954, 4836, 7045, 3564, 13958, 6965, 3940, 9260, 13660, 14241, 3367, 8521, 8574, 6777, 5845, 12639, 12480, 12167, 6664, 5873, 5735, 11245, 2940, 11407, 6270, 8297, 11186, 4423, 16388, 5731, 12850, 7725, 7315, 7030, 9402, 5513, 12880, 17822, 5240, 12854, 15200, 3633, 2678, 19801, 4877, 13496, 6457, 12304, 1926, 7095, 7280, 7262, 7079, 2714, 4650, 3018, 10638, 6465, 11658, 2008, 4635, 12481, 4038, 12773, 4876, 6035, 9234, 3786, 12197, 3286, 5089, 3769, 17808, 8257, 11543, 3553, 6945, 5222, 8033, 10611, 6824, 7831, 6781, 9189, 9190, 10615, 6826, 9246, 12028, 15656, 10747, 7375, 3477, 6450, 6863, 9291, 7804, 9160, 10454, 10415, 10404, 10500, 4705, 4709, 7817, 7814, 7809, 7807, 7809, 7803, 7809, 7880, 7857, 3943, 5012, 6073, 8237, 5811, 12154, 12154, 12219, 12259, 7744, 3266, 10801, 15164, 14211, 14167, 4915, 1888, 1890, 539, 7147, 7251, 7288, 5784, 3631, 2898, 7255, 7, 7072, 6890, 6889, 5277, 5208, 17791, 17791, 17870, 12020, 12067, 12075, 3684, 11078, 10888, 48]\n"
     ]
    }
   ],
   "source": [
    "# Number of *unique* words in the books after data pre-processing\n",
    "unique_words = list()\n",
    "for book in docs:\n",
    "    fdist_doc = nltk.FreqDist(book)\n",
    "    filtered_word_freq = dict((word, freq) for word, freq in fdist_doc.items() if not word.isdigit())\n",
    "    unique_words.append(len(filtered_word_freq))\n",
    "print(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d384ac2f-d57f-4e87-9338-8d1f3ded7035",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(docs_stem)\n",
    "# print(dictionary)\n",
    "token_to_id = dictionary.token2id\n",
    "print(type(token_to_id))\n",
    "# print(token_to_id)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "639bcd28-c400-453c-abaf-1a6a63c1d6f6",
   "metadata": {},
   "source": [
    "# Get the top-20 most frequent words in the file. User FreqDist an most_common\n",
    "wordcloud = WordCloud(width=1600, height=800, background_color='white').generate(str(docs_stem))\n",
    "fig = plt.figure(figsize=(30,10), facecolor='white')\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.title('Top 20 Most Common Words', fontsize=100)\n",
    "plt.tight_layout(pad=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f9b54e4c-eac0-4c7c-979f-f7269731403e",
   "metadata": {},
   "source": [
    "fdist_sg = FreqDist(justice_stemmed).most_common(25)\n",
    "# print(fdist_sg.most_common(20))\n",
    "\n",
    "x, y = zip(*fdist_sg)\n",
    "plt.figure(figsize=(50,30))\n",
    "plt.margins(0.02)\n",
    "plt.bar(x, y)\n",
    "plt.xlabel('Words', fontsize=50)\n",
    "plt.ylabel('Frequency of Words', fontsize=50)\n",
    "plt.yticks(fontsize=40)\n",
    "plt.xticks(rotation=60, fontsize=40)\n",
    "plt.title('Frequency of 25 Most Common Words', fontsize=60)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aec044a-1867-466e-903d-61f19cde0144",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
