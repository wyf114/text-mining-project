{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "821dfbe8",
   "metadata": {},
   "source": [
    "# Split PDF Book into Multiple Text File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2c81dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PyPDF2 import PdfReader, PdfWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ff33b596",
   "metadata": {},
   "outputs": [],
   "source": [
    "book = PdfReader('Mining Text Data.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "3d1cb896",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chapter 7\n",
      "TRANSFER LEARNING FOR TEXT\n",
      "MINING\n",
      "Weike Pan\n",
      "Hong Kong University of Science and Technology\n",
      "Clearwater Bay, Kowloon, Hong Kong\n",
      "weikep@cse.ust.hk\n",
      "Erheng Zhong\n",
      "Hong Kong University of Science and Technology\n",
      "Clearwater Bay, Kowloon, Hong Kong\n",
      "ezhong@cse.ust.hk\n",
      "Qiang Yang\n",
      "Hong Kong University of Science and Technology\n",
      "Clearwater Bay, Kowloon, Hong Kong\n",
      "qyang@cse.ust.hk\n",
      "Abstract Over the years, transfer learning has received much attention in machine\n",
      "learning research and practice. Researchers have found that a major\n",
      "bottleneck associated with machine learning and text mining is the lack\n",
      "of high-quality annotated examples to help train a model. In response,\n",
      "transfer learning oﬀers an attractive solution for this problem. Various\n",
      "transfer learning methods are designed to extract the useful knowledge\n",
      "from diﬀerent but related auxiliary domains. In its connection to text\n",
      "mining, transfer learning has found novel and useful applications. In\n",
      "this chapter, we will review some most recent developments in transfer\n",
      "learningfortextmining, explainrelatedalgorithmsindetail, andproject\n",
      "future developments of this ﬁeld. We focus on two important topics:\n",
      "cross-domain text document classiﬁcation and heterogeneous transfer\n",
      "learning that uses labeled text documents to help classify images.\n",
      "Keywords: Transfer learning, text mining, classiﬁcation, clustering, learning-to-\n",
      "rank.\n",
      "© Springer Science+Business Media, LLC 2012 223  C.C. Aggarwal and C.X. Zhai(eds.),Mining Text Data , DOI 10.1007/978-1-4614-3223-4_7,\n"
     ]
    }
   ],
   "source": [
    "# actual page number\n",
    "actual_page = 12\n",
    "chapter_first_page = [1,11,42,76,128,161,221,256,293,318,355,379,409,458,513]\n",
    "chapter_first_page[0]+actual_page\n",
    "page = book.pages[chapter_first_page[6]+actual_page]\n",
    "print(page.extract_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615f020f",
   "metadata": {},
   "source": [
    "- Chapter 1: 1-11\n",
    "- Chapter 2: 11-42\n",
    "- Chapter 3: 42-76\n",
    "- Chapter 4: 76-128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "ce53f9e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13, 23, 54, 88, 140, 173, 233, 268, 305, 330, 367, 391, 421, 470, 525]\n"
     ]
    }
   ],
   "source": [
    "book_page_index = [x+12 for x in chapter_first_page]\n",
    "print(book_page_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "2e2f311a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting at page 13, ending at page 23\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "starting at page 23, ending at page 54\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "starting at page 54, ending at page 88\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "starting at page 88, ending at page 140\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "starting at page 140, ending at page 173\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "starting at page 173, ending at page 233\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "starting at page 233, ending at page 268\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "starting at page 268, ending at page 305\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "starting at page 305, ending at page 330\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "starting at page 330, ending at page 367\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "starting at page 367, ending at page 391\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "starting at page 391, ending at page 421\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "starting at page 421, ending at page 470\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "starting at page 470, ending at page 525\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n",
      "512\n",
      "513\n",
      "514\n",
      "515\n",
      "516\n",
      "517\n",
      "518\n",
      "519\n",
      "520\n",
      "521\n",
      "522\n",
      "523\n",
      "524\n",
      "525\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(book_page_index)-1):\n",
    "    print(f'starting at page {book_page_index[i]}, ending at page {book_page_index[i+1]}')\n",
    "    start_page = book_page_index[i]\n",
    "    end_page = book_page_index[i+1]\n",
    "    num_pages = end_page - start_page + 1\n",
    "    content = ''\n",
    "    for j in range(num_pages):\n",
    "        print(j+start_page)\n",
    "        content += book.pages[j+start_page].extract_text()\n",
    "    with open(f\"doc_{i+1}.txt\", \"w\", encoding='utf-8') as file:\n",
    "        file.write(content)\n",
    "        file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93f73a7",
   "metadata": {},
   "source": [
    "# Text Similarity (Chapter Similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4af6ccd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import *\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "469b321b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['doc_1.txt', 'doc_10.txt', 'doc_11.txt', 'doc_12.txt', 'doc_13.txt', 'doc_14.txt', 'doc_2.txt', 'doc_3.txt', 'doc_4.txt', 'doc_5.txt', 'doc_6.txt', 'doc_7.txt', 'doc_8.txt', 'doc_9.txt']\n"
     ]
    }
   ],
   "source": [
    "corpus = PlaintextCorpusReader('data', '.+\\.txt')\n",
    "fids = corpus.fileids()\n",
    "print(fids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f26febc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(11947 unique tokens: ['abil', 'abl', 'abstract', 'access', 'account']...)\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary from book\n",
    "book_docs = [corpus.words(f) for f in fids]\n",
    "book_lower = [[w.lower() for w in doc] for doc in book_docs]\n",
    "book_alpha = [[w for w in doc if re.search('^[a-z]+$',w)] for doc in book_lower]\n",
    "stop_list = stopwords.words('english')            \n",
    "book_stop = [[w for w in doc if w not in stop_list] for doc in book_alpha]\n",
    "stemmer = PorterStemmer()\n",
    "book_stem = [[stemmer.stem(w) for w in doc] for doc in book_stop]\n",
    "book_dict = corpora.Dictionary(book_stem)\n",
    "print(book_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25800357",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all doc to list of sparse vectors\n",
    "book_vecs = [book_dict.doc2bow(doc) for doc in book_stem]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5003bfd4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from gensim import similarities\n",
    "# Build an index from a list of sparse vectors\n",
    "index = similarities.SparseMatrixSimilarity(book_vecs,11947)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "651ee498",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.9999979), (1, 0.55079097), (2, 0.53323555), (3, 0.63295704), (4, 0.37265885), (5, 0.5907426), (6, 0.4736471), (7, 0.43834755), (8, 0.5010129), (9, 0.42730898), (10, 0.60467124), (11, 0.5869448), (12, 0.48561707), (13, 0.640863)]\n"
     ]
    }
   ],
   "source": [
    "# e.g. find similar chapter for chapter 1\n",
    "chapter_1 = book_vecs[0]\n",
    "sims = index[chapter_1]\n",
    "print(list(enumerate(sims)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a83f3234",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e907744",
   "metadata": {},
   "source": [
    "# Text Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bc2a2f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chapter 1\n",
      "AN INTRODUCTION TO TEXT MINING\n",
      "Charu C. Aggarwal\n",
      "IBM T. J. Watson Research Center\n",
      "Yorktown Heights, NY\n",
      "charu@us.ibm.com\n",
      "ChengXiang Zhai\n",
      "University of Illinois at Urbana-Champaign\n",
      "Urbana, IL\n",
      "czhai@cs.uiuc.edu\n",
      "Abstract\n",
      "The problem of text mining has gained increasing attention in recent\n",
      "years because of the large amounts of text data, which are created in\n",
      "a variety of social network, web, and other information-centric applica-\n",
      "tions. Unstructureddataistheeasiestformofdatawhichcanbecreated\n",
      "in any application scenario. As a result, there has been a tremendous\n",
      "need to design methods and algorithms which can eﬀectively process a\n",
      "wide variety of text applications. This book will provide an overview\n",
      "of the diﬀerent methods and algorithms which are common in the text\n",
      "domain, with a particular focus on mining methods.\n",
      "1. Introduction\n",
      "Datamining isaﬁeld whichhasseen rapid advancesin recentyears[8]\n",
      "because of the immense advances in hardware and software technology\n",
      "which has lead to the availability of diﬀerent kinds of data. This is\n",
      "particularly true for the case of text data, where the development of\n",
      "hardware and software platforms for the web and social networks has\n",
      "enabled the rapid creation of large repositories of diﬀerent kinds of data.\n",
      "In particular, the web is a technological enabler which encourages the\n",
      " \n",
      "© Springer Science+Business Media, LLC 2012 1  C.C. Aggarwal and C.X. Zhai(eds.),Mining Text Data , DOI 10.1007/978-1-4614-3223-4_1,2 MINING TEXT DATA\n",
      "creation of a large amount of text content by diﬀerent users in a form\n",
      "which is easy to store and process. The increasing amounts of text dataavailable from diﬀerent applications has created a need for advances in\n",
      "algorithmic design which can learn interesting patterns from the data in\n",
      "a dynamic and scalable way.\n",
      "While structured data is generally managed with a database system,\n",
      "text data is typically managed via a search engine due to the lack ofstructures [5]. A search engine enables a user to ﬁnd useful informa-tion from a collection conveniently with a keyword query, and how toimprove the eﬀectiveness and eﬃciency of a search engine has been a\n",
      "central research topic in the ﬁeld of information retrieval [13, 3], where\n",
      "many related topics to search such as text clustering, text categoriza-tion, summarization, and recommender systems are also studied [12, 9,7].\n",
      "However, research in information retrieval has traditionally focused\n",
      "more on facilitating information access [13] rather than analyzing infor-mation to discover patterns, which is the primary goal of text mining.\n",
      "The goal of information access is to connect the right information with\n",
      "the right users at the right time with less emphasis on processing ortransformation of text information. Text mining can be regarded as go-ing beyond information access to further help users analyze and digestinformation and facilitate decision making.There are also many applica-tions of text mining where the primary goal is to analyze and discoverany interesting pattterns, including trends and outliers, in text data,\n",
      "and the notion of a query is not essential or even relevant.\n",
      "Technically, mining techniques focus on the primary models, algo-\n",
      "rithms and applications about what one can learn from diﬀerent kindsof text data. Some examples of such questions are as follows:\n",
      "What are the primary supervised and unsupervised models forlearning from text data? How are traditional clustering and clas-siﬁcation problems diﬀerent for text data, as compared to the tra-\n",
      "ditional database literature?\n",
      "What are the useful tools and techniques used for mining text\n",
      "data? Which are the useful mathematical techniques which oneshould know, and which are repeatedly used in the context of dif-ferent kinds of text data?\n",
      "What are the key application domains in which such mining tech-niques are used, and how are they eﬀectively applied?\n",
      "A number of key characteristics distinguish text data from other forms\n",
      "of data such as relational or quantitative data. This naturally aﬀects theAn Introduction to Text Mining 3\n",
      "mining techniques which can be used for such data. The most important\n",
      "characteristic of text data is that it is sparseandhigh dimensional .F o r\n",
      "example, a given corpus may be drawn from a lexicon of about 100,000\n",
      "words, butagiventextdocumentmaycontainonlyafewhundredwords.\n",
      "Thus, a corpus of text documents can be represented as a sparse term-\n",
      "document matrix of sizen×d,w h e nnis the number of documents, and\n",
      "dis the size of the lexicon vocabulary. The ( i,j)th entry of this matrix\n",
      "is the (normalized) frequency of the jth word in the lexicon in document\n",
      "i. The large size and the sparsity of the matrix has immediate implica-\n",
      "tions for a number of data analytical techniques such as dimensionality\n",
      "reduction. In such cases, the methods for reduction should be speciﬁ-\n",
      "cally designed while taking this characteristic of text data into account.The variation in word frequencies and document lengths also lead to anumber of issues involving document representation and normalization,which are critical for text mining.\n",
      "Furthermore, text data can be analyzed at diﬀerent levels of represen-\n",
      "tation. For example, text data can easily be treated as a bag-of-words,\n",
      "or it can be treated as a string of words. However, in most applica-\n",
      "tions, it would be desirable to represent text information semantically\n",
      "so that more meaningful analysis and mining can be done. For exam-ple, representing text data at the level of named entities such as people,organizations, and locations, and their relations may enable discoveryof more interesting patterns than representing text as a bag of words.Unfortunately, the state of the art methods in natural language process-\n",
      "ing are still not robust enough to work well in unrestricted text domains\n",
      "to generate accurate semantic representation of text. Thus most textmining approaches currently still rely on the more shallow word-basedrepresentations, especially the bag-of-wrods approach, which, while los-ing the positioning information in the words, is generally much simplerto deal with from an algorithmic point of view than the string-basedapproach. In special domains (e.g., biomedical domain) and for special\n",
      "mining tasks (e.g., extraction of knowledge from the Web), natural lan-\n",
      "guage processing techniques, especially information extraction, are alsoplaying an important role in obtaining a semantically more meaningfulrepresentation of text.\n",
      "Recently, there has been rapid growth of text data in the context\n",
      "of diﬀerent web-based applications such as social media, which oftenoccur in the context of multimedia or other heterogeneous data domains.\n",
      "Therefore, a number of techniques have recently been designed for the\n",
      "joint mining of text data in the context of these diﬀerent kinds of data\n",
      "domains. For example, the Web contains text and image data whichare often intimately connected to each other and these links can be used4 MINING TEXT DATA\n",
      "to improve the learning process from one domain to another. Similarly,\n",
      "cross-lingual linkages between documents of diﬀerent languages can alsobe used in order to transfer knowledge from one language domain to\n",
      "another. This is closely related to the problem of transfer learning [11].\n",
      "The rest of this chapter is organized as follows. The next section\n",
      "will discuss the diﬀerent kinds of algorithms and applications for textmining. We will also point out the speciﬁc chapters in which they arediscussed in the book. Section 3 will discuss some interesting futureresearch directions.\n",
      "2. Algorithms for Text Mining\n",
      "In this section, we will explore the key problems arising in the con-\n",
      "text of text mining. We will also present the organization of the diﬀerent\n",
      "chapters of this book in the context of these diﬀerent problems. We in-tentionally leave the deﬁnition of the concept ”text mining” vague tobroadly cover a large set of related topics and algorithms for text anal-ysis, spanning many diﬀerent communities, including natural languageprocessing, information retrieval, data mining, machine learning, andmany application domains such as the World Wide Web and Biomedi-\n",
      "cal Science. We have also intentionally allowed (sometimes signiﬁcant)\n",
      "overlaps between chapters to allow each chapter to be relatively selfcontained, thus useful as a standing-alone chapter for learning about aspeciﬁc topic.Information Extraction from Text Data: Information Extraction\n",
      "is one of the key problems of text mining, which serves as a startingpoint for many text mining algorithms. For example, extraction of enti-\n",
      "ties and their relations from text can reveal more meaningful semantic\n",
      "information in text data than a simple bag-of-words representation, andis generally needed to support inferences about knowledge buried in textdata. Chapter 2 provides an survey of key problems in Information Ex-traction and the major algorithms for extracting entities and relationsfrom text data.Text Summarization: Another common function needed in many text\n",
      "mining applications is to summarize the text documents in order to ob-\n",
      "tain a brief overview of a large text document or a set of documents ona topic. Summarization techniques generally fall into two categories. Inextractive summarization, a summary consists of information units ex-tracted from the original text; in contrast, in abstractive summarization,a summary may contain “synthesized” information units that may notnecessarily occur in the text documents. Most existing summarization\n",
      "methods are extractive, and in Chapter 3, we give a brief survey of theseAn Introduction to Text Mining 5\n",
      "commonly used summarization methods.\n",
      "Unsupervised Learning Methods from Text Data: Unsupervised\n",
      "learning methods do not require any training data, thus can be applied\n",
      "to any text data without requiring any manual eﬀort. The two main un-\n",
      "supervised learning methods commonly used in the context of text dataareclustering andtopic modeling . The problem of clustering is that\n",
      "of segmenting a corpus of documents into partitions, each correspond-ing to a topical cluster. The problems of clustering and topic modelingare closely related. In topic modeling we use a probabilistic model inorder to determine a softclustering, in which each document has a\n",
      "membership probability of the cluster, as opposed to a hard segmenta-\n",
      "tion of the documents. Topic models can be considered as the processof clustering with a generative probabilistic model. Each topiccan be\n",
      "considered a probability distribution over words, with the representativewords having the highest probability. Each document can be expressedas a probabilistic combination of these diﬀerent topics. Thus, a topiccan be considered to be analogous to a cluster, and the membership\n",
      "of a document to a cluster is probabilistic in nature. This also leads\n",
      "to a more elegant cluster membership representation in cases in whichthe document is known to contain distinct topics. In the case of hardclustering, it is sometimes challenging to assign a document to a sin-gle cluster in such cases. Furthermore, topic modeling relates elegantlyto the dimension reduction problem, where each topic provides a con-ceptual dimension, and the documents may be represented as a linear\n",
      "probabilistic combination of these diﬀerent topics. Thus, topic-modeling\n",
      "provides an extremely general framework, which relates to both the clus-tering and dimension reduction problems. In chapter 4, we study theproblem of clustering, while topic modeling is covered in two chapters(Chapters 5 and 8). In Chapter 5, we discuss topic modeling from theperspective of dimension reduction since the discovered topics can serveas a low-dimensional space representation of text data, where semanti-\n",
      "cally related words can “match” each other, which is hard to achieve\n",
      "with bag-of-words representation. In chapter 8, topic modeling is dis-cussed as a general probabilistic model for text mining.LSI and Dimensionality Reduction for Text Mining: The prob-\n",
      "lem of dimensionality reduction is widely studied in the database liter-ature as a method for representing the underlying data in compressedformat for indexing and retrieval [10]. A variation of dimensionality re-\n",
      "duction which is commonly used for text data is known as latent seman-\n",
      "tic indexing [6]. One of the interesting characteristics of latent semantic\n",
      "indexing is that it brings our the key semantic aspects of the text data,whichmakesitmoresuitableforavarietyofminingapplications. Forex-6 MINING TEXT DATA\n",
      "ample, the noise eﬀects of synonymy and polysemy are reduced because\n",
      "of the use of such dimensionality reduction techniques. Another familyof dimension reduction techniques are probabilistic topic models,notably\n",
      "PLSA, LDA, and their variants; they perform dimension reduction in a\n",
      "probabilistic way with potentially more meaningful topic representationsbased on word distributions. In chapter 5, we will discuss a variety ofLSI and dimensionality reduction techniques for text data, and their usein a variety of mining applications.Supervised Learning Methods for Text Data: Supervised learning\n",
      "methods are general machine learning methods that can exploit train-\n",
      "ing data (i.e., pairs of input data points and the corresponding desired\n",
      "output) to learn a classiﬁer or regression function that can be used tocompute predictions on unseen new data. Since a wide range of applica-tion problems can be cast as a classiﬁcation problem (that can be solvedusing supervised learning), the problem of supervised learning is some-times also referred to as classiﬁcation. Most of the traditional methodsfor text mining in the machine learning literature have been extended\n",
      "to solve problems of text mining. These include methods such as rule-\n",
      "based classiﬁer, decision trees, nearest neighbor classiﬁers, maximum-margin classiﬁers, and probabilistic classiﬁers. In Chapter 6, we willstudy machine learning methods for automated text categorization, amajor application area of supervised learning in text mining. A moregeneral discussion of supervised learning methods is given in Chapter 8.A special class of techniques in supervised learning to address the issue\n",
      "of lack of training data, called transfer learning , are covered in Chapter\n",
      "7.Transfer Learning with Text Data: The afore-mentioned example\n",
      "of cross-lingual mining provides a case where the attributes of the textcollection may be heterogeneous. Clearly, the feature representations inthe diﬀerent languages are heterogeneous, and it can often provide use-ful to transfer knowledge from one domain to another, especially when\n",
      "their is paucity of data in one domain. For example, labeled English\n",
      "documents are copious and easy to ﬁnd. On the other hand, it is muchharder to obtain labeled Chinese documents. The problem of transferlearning attempts to transferthe learned knowledge from one domain to\n",
      "another. Some other scenarios in which this arises is the case where wehave a mixture of text and multimedia data. This is often the case inmany web-based and social media applications such as Flickr,Youtube\n",
      "or other multimedia sharing sites. In such cases, it may be desirable to\n",
      "transfer the learned knowledge from one domain to another with the useof cross-media transfer. Chapter 7 provides a detailed survey of suchlearning techniques.An Introduction to Text Mining 7\n",
      "Probabilistic Techniques for Text Mining: A variety of probabilis-\n",
      "tic methods, particularly unsupervised topic models such as PLSA andLDA and supervised learning methods such as conditional random ﬁelds\n",
      "are used frequently in the context of text mining algorithms. Since such\n",
      "methods are used frequently in a wide variety of contexts, it is usefulto create an organized survey which describes the diﬀerent tools andtechniques that are used in this context. In Chapter 8, we introducethe basics of the common probabilistic models and methods which areoften used in the context of text mining. The material in this chapter isalso relevant to many of the clustering, dimensionality reduction, topic\n",
      "modeling and classiﬁcation techniques discussed in Chapters 4, 5, 6 and\n",
      "7.Mining Text Streams: Many recent applications on the web create\n",
      "massive streams of text data. In particular web applications such associal networks which allow the simultaneous input of text from a widevariety of users can result in a continuous stream of large volumes oftext data. Similarly, news streams such as Reutersor aggregators such\n",
      "asGoogle news create large volumes of streams which can be mined con-\n",
      "tinuously. Such text data are more challenging to mine, because theyneed to be processed in the context of a one-pass constraint [1]. Theone-pass constraint essentially means that it may sometimes be diﬃcultto store the data oﬄine for processing, and it is necessary to performthe mining tasks continuously, as the data comes in. This makes algo-rithmic design a much more challenging task. In chapter 9, we study\n",
      "the common techniques which are often used in the context of a variety\n",
      "of text mining tasks.Cross-Lingual Mining of Text Data: With the proliferation of web-\n",
      "based and other information retrieval applications to other applications,it has become particularly useful to apply mining tasks in diﬀerent lan-guages, or use the knowledge or corpora in one language to another.For example, in cross-language mining, it may be desirable to cluster a\n",
      "group of documents in diﬀerent languages, so that documents from dif-\n",
      "ferent languages but similar semantic topics may be placed in the samecluster. Such cross-lingual applications are extremely rich, because theycan often be used to leverage knowledge from one data domain into an-other. In chapter 10, we will study methods for cross-lingual mining oftext data, covering techniques such as machine translation, cross-lingualinformation retrieval, and analysis of comparable and parallel corpora.\n",
      "Text Mining in Multimedia Networks: Text often occurs in the\n",
      "context of many multimedia sharing sites such as FlickrorYoutube.\n",
      "A natural question arises as to whether we can enrich the underlyingmining process by simultaneously using the data from other domains8 MINING TEXT DATA\n",
      "together with the text collection. This is also related to the problem of\n",
      "transfer learning, which was discussed earlier. In chapter 11, a detailedsurvey will be provided on mining other multimedia data together with\n",
      "text collections.\n",
      "Text Mining in Social Media: One of the most common sources of\n",
      "text on the web is the presence of social media, which allows humanactors to express themselves quickly and freely in the context of a widerange of subjects [2]. Social media is now exploited widely by commer-cial sites for inﬂuencing users and targeted marketing. The process ofmining text in social media requires the special ability to mine dynamic\n",
      "data which often contains poor and non-standard vocabulary. Further-\n",
      "more, the text may occur in the context of linked social networks. Suchlinks can be used in order to improve the quality of the underlying min-ing process. For example, methods that use both link and content [4]are widely known to provide much more eﬀective results which use onlycontent or links. Chapter 12 provides a detailed survey of text miningmethods in social media.\n",
      "Opinion Mining from Text Data: A considerable amount of text on\n",
      "web sites occurs in the context of product reviews or opinions of diﬀerentusers. Mining such opinionated text data to reveal and summarize theopinions about a topic has widespread applications, such as in support-ing consumers for optimizing decisions and business intelligence. spamopinions which are not useful and simply add noise to the mining pro-cess. Chapter 13 provides a detailed survey of models and methods for\n",
      "opinion mining and sentiment analysis.\n",
      "Text Mining from Biomedical Data: Text mining techniques play\n",
      "an important role in both enabling biomedical researchers to eﬀectivelyand eﬃciently access the knowledge buried in large amounts of literatureand supplementing the mining of other biomedical data such as genomesequences, gene expression data, and protein structures to facilitate andspeed up biomedical discovery. As a result, a great deal of research work\n",
      "has been done in adapting and extending standard text mining methods\n",
      "to the biomedical domain, such as recognition of various biomedical en-tities and their relations, text summarization, and question answering.Chapter 14 provides a detailed survey of the models and methods usedfor biomedical text mining.\n",
      "3. Future Directions\n",
      "The rapid growth of online textual data creates an urgent need for\n",
      "powerful text mining techniques. As an interdisciplinary ﬁeld, text data\n",
      "mining spans multiple research communities, especially data mining,An Introduction to Text Mining 9\n",
      "natural language processing, information retrieval, and machine learn-\n",
      "ing with applications in many diﬀerent areas, and has attracted muchattention recently. Many models and algorithms have been developed\n",
      "for various text mining tasks have been developed as we discussed above\n",
      "and will be surveyed in the rest of this book.\n",
      "Looking forward, we see the following general future directions that\n",
      "are promising:\n",
      "Scalable and robust methods for natural language under-standing: Understanding text information is fundamental to text\n",
      "mining. While the current approaches mostly rely on bag of wordsrepresentation, it is clearly desirable to go beyond such a simple\n",
      "representation. Informationextractiontechniquesprovideonestep\n",
      "forward toward semantic representation, but the current informa-tion extraction methods mostly rely on supervised learning andgenerally only work well when suﬃcient training data are avail-able, restricting its applications. It is thus important to developeﬀective and robust information extraction and other natural lan-guage processing methods that can scale to multiple domains.\n",
      "Domain adaptation and transfer learning: Many text min-\n",
      "ing tasks rely on supervised learning, whose eﬀectiveness highlydepends on the amount of training data available. Unfortunately,it is generally labor-intensive to create large amounts of trainingdata. Domain adaptation and transfer learning methods can al-leviate this problem by attempting to exploit training data thatmight be available in a related domain or for a related task. How-\n",
      "ever, the current approaches still have many limitations and are\n",
      "generally inadequate when there is no or little training data inthe target domain. Further development of more eﬀective domainadaptation and transfer learning methods is necessary for moreeﬀective text mining.\n",
      "Contextual analysis of text data: Textdataisgenerallyassoci-\n",
      "atedwithalotofcontextinformationsuchasauthors, sources, and\n",
      "time, or more complicated information networks associated with\n",
      "text data. In many applications, it is important to consider thecontext as well as user preferences in text mining. It is thus impor-tant to further extend existing text mining approaches to furtherincorporate context and information networks for more powerfultext analysis.\n",
      "Parallel text mining: In many applications of text mining, the\n",
      "amount of text data is huge and is likely increasing over time,10 MINING TEXT DATA\n",
      "thus it is infeasible to store the data in one machine, making it\n",
      "necessary to develop parallel text mining algorithms that can runon a cluster of computers to perform text mining tasks in parallel.\n",
      "Inparticular, howtoparallelizeallkindsoftextminingalgorithms,\n",
      "including both unsupervised and supervised learning methods is amajor future challenge. This direction is clearly related to cloudcomputing and data-intensive computing, which are growing ﬁeldsthemselves.\n",
      "References\n",
      "[1] C. Aggarwal. Data Streams: Models and Algorithms , Springer, 2007.\n",
      "[2] C. Aggarwal. Social Network Data Analytics , Springer, 2011.\n",
      "[3] R. A. Baeza-Yates, B. A. Ribeiro-Neto, Modern Information Re-\n",
      "trieval - the concepts and technology behind search, Second edition ,\n",
      "Pearson Education Ltd., Harlow, England, 2011.\n",
      "[4] S. Chakrabarti, B. Dom, P. Indyk. Enhanced Hypertext Categoriza-\n",
      "tion using Hyperlinks, ACM SIGMOD Conference , 1998.\n",
      "[5] W. B. Croft, D. Metzler, T. Strohma, Search Engines - Information\n",
      "Retrieval in Practice , Pearson Education, 2009.\n",
      "[6] S. Deerwester, S. Dumais, T. Landauer, G. Furnas, R. Harshman.\n",
      "Indexing by Latent Semantic Analysis. JASIS, 41(6), pp. 391–407,\n",
      "1990.\n",
      "[7] D. A. Grossman, O. Frieder, Information Retrieval: Algorithms and\n",
      "Heuristics (The Kluwer International Series on Information Re-trieval), Springer-Verlag New York, Inc, 2004.\n",
      "[ 8 ]J .H a n ,M .K a m b e r . Data Mining: Concepts and Techniques ,2 n d\n",
      "Edition, Morgan Kaufmann, 2005.\n",
      "[9] C. Manning, P. Raghavan, H. Schutze, Introduction to Information\n",
      "Retrieval , Cambridge University Press, 2008.\n",
      "[10] I. T. Jolliﬀee. Principal Component Analysis. Springer, 2002.\n",
      "[11] S. J. Pan, Q. Yang. A Survey on Transfer Learning, IEEE Trans-\n",
      "actions on Knowledge and Data Engineering , 22(10): pp 1345–1359,\n",
      "Oct. 2010.\n",
      "[12] G. Salton. An Introduction to Modern Information Retrieval ,M c\n",
      "Graw Hill, 1983.\n",
      "[13] K. Sparck Jones P. Willett (ed.). Readings in Information Retrieval ,\n",
      "Morgan Kaufmann Publishers Inc, 1997.Chapter 2\n",
      "INFORMATION EXTRACTION FROM\n",
      "TEXT\n",
      "Jing Jiang\n",
      "Singapore Management University\n",
      "jingjiang@smu.edu.sg\n",
      "Abstract Informationextractionisthetaskofﬁndingstructuredinformationfrom\n",
      "unstructured or semi-structured text. It is an important task in text\n",
      "mining and has been extensively studied in various research commu-\n",
      "nities including natural language processing, information retrieval and\n",
      "Web mining. It has a wide range of applications in domains such as\n",
      "biomedical literature mining and business intelligence. Two fundamen-\n",
      "tal tasks of information extraction are named entity recognition and\n",
      "relation extraction. The former refers to ﬁnding names of entities such\n",
      "as people, organizations and locations. The latter refers to ﬁnding the\n",
      "semanticrelationssuchas FounderOf andHeadquarteredIn betweenen-\n",
      "tities. In this chapter we provide a survey of the major work on named\n",
      "entity recognition and relation extraction in the past few decades, with\n",
      "a focus on work from the natural language processing community.\n",
      "Keywords: Information extraction, named entity recognition, relation extraction\n",
      "1. Introduction\n",
      "Information extraction from text is an important task in text min-\n",
      "ing. The general goal of information extraction is to discover structured\n",
      "information from unstructured or semi-structured text. For example,\n",
      "given the following English sentence,\n",
      "In 1998, Larry Page and Sergey Brin founded Google Inc.\n",
      "we can extract the following information,\n",
      "FounderOf( Larry Page ,Google Inc. ),\n",
      "FounderOf( Sergey Brin ,Google Inc. ),\n",
      "FoundedIn( Google Inc. ,1998 ).\n",
      "© Springer Science+Business Media, LLC 2012 11  C.C. Aggarwal and C.X. Zhai(eds.),Mining Text Data , DOI 10.1007/978-1-4614-3223-4_2,\n"
     ]
    }
   ],
   "source": [
    "#open text file in read mode\n",
    "text_file = open(\"data/doc_1.txt\", \"r\", encoding='utf-8')\n",
    " \n",
    "#read whole file to a string\n",
    "data = text_file.read()\n",
    " \n",
    "#close file\n",
    "text_file.close()\n",
    " \n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "010472c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the of text in and to data mining is for\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "# Load the text to be summarized\n",
    "text = data\n",
    "\n",
    "# Tokenize the text into individual words\n",
    "tokens = gensim.utils.simple_preprocess(text)\n",
    "\n",
    "# Create a dictionary from the tokens\n",
    "dictionary = corpora.Dictionary([tokens])\n",
    "\n",
    "# Create a corpus from the dictionary and tokens\n",
    "corpus = [dictionary.doc2bow(tokens)]\n",
    "\n",
    "# Train the LDA model with the corpus\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=2, passes=10)\n",
    "\n",
    "# Get the top sentences from the LDA model\n",
    "top_sentences = sorted(lda_model.show_topic(0, topn=10), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Generate the summary from the top sentences\n",
    "summary = ' '.join([sent[0] for sent in top_sentences])\n",
    "\n",
    "# Print the summary\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "587258e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
