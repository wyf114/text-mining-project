Chapter 11
TEXT MINING IN MULTIMEDIA
Zheng-Jun Zha
School of Computing, National University of Singapore
zhazj@comp.nus.edu.sg
Meng Wang
School of Computing, National University of Singapore
wangm@comp.nus.edu.sg
Jialie Shen
Singapore Management University
jlshen@smu.edu.sg
Tat-Seng Chua
School of Computing, National University of Singapore
chuats@comp.nus.edu.sg
Abstract A large amount of multimedia data (e.g., image and video) is now avail-
able on the Web. A multimedia entity does not appear in isolation,
but is accompanied by various forms of metadata, such as surround-
ing text, user tags, ratings, and comments etc. Mining these textual
metadata has been found to be eï¬€ective in facilitating multimedia in-
formation processing and management. A wealth of research eï¬€orts has
been dedicated to text mining in multimedia. This chapter provides a
comprehensive survey of recent research eï¬€orts. Speciï¬cally, the survey
focuses on four aspects: (a) surrounding text mining; (b) tag mining;
(c) joint text and visual content mining; and (d) cross text and visual
content mining. Furthermore, open research issues are identiï¬ed based
on the current research eï¬€orts.
Keywords: Text Mining, Multimedia, Surrounding Text, Tagging, Social Network
Â© Springer Science+Business Media, LLC 2012 361  C.C. Aggarwal and C.X. Zhai(eds.),Mining Text Data , DOI 10.1007/978-1-4614-3223-4_11,362 MINING TEXT DATA
Ñ„ÅÅµÅÆÆŒÄÑÎ—ÅšÆšÆšÆ‰Í—Í¬Í¬
ÇÇÇÍ˜ÄÄ‚ÆŒÆÅµÄ‚ÅÇŒÍ˜ÄÅ½ÅµÍ¬ÇÆ‰Í²ÄÅ½Å¶ÆšÄÅ¶ÆšÍ¬ÆµÆ‰Å¯Å½Ä‚ÄšÆÍ¬Ï®Ï¬Ï­Ï­Í¬Ï¬Ï­Í¬Ï®Ï¬Ï­Ï­Í²>Ä‚ÅµÄÅ½ÆŒÅÅšÅÅ¶ÅÍ²'Ä‚Å¯Å¯Ä‚ÆŒÄšÅ½Í²>WÏ±Ï²Ï¬Í²Ï°Í²ÅÄÅ½Å¯Å½ÆŒÄÍ²&ÆŒÅ½Å¶ÆšÍ²^ÅÄšÄÍ²sÅÄÇÍ²Ï±Ï³Ï¬Ç†Ï°Ï¬Ï±Í˜Å©Æ‰ÅÎ—Ä‚Å¯ÆšÑÎ—Ï®Ï¬Ï­Ï­>Ä‚ÅµÄÅ½ÆŒÅÅšÅÅ¶Å'Ä‚Å¯Å¯Ä‚ÆŒÄšÅ½>WÏ±Ï²Ï¬Í²Ï°ÅÄÅ½Å¯Å½ÆŒÄÎ—ÄÅ¯Ä‚ÆÆÑÎ—Ä‚Å¯ÅÅÅ¶ÆŒÅÅÅšÆšÎ—Ñ…WÄ‚ÅÄdÅÆšÅ¯Ä
^ÆµÆŒÆŒÅ½ÆµÅ¶ÄšÅÅ¶ÅdÄÇ†Æš
^ÆµÆŒÆŒÅ½ÆµÅ¶ÄšÅÅ¶ÅdÄÇ†ÆšÅ¯ÆšÆšÄÇ†Æš
Figure 11.1. Illustration of textual metadata of an embedded image in a Web page.
1. Introduction
Lower cost hardware and growing communications infrastructure (e.g.
Web, cell Phones, etc.) have led to an explosion in the availability of
ubiquitous devices to produce, store, view and exchange multimedia en-tities (images, videos). A large amount of image and video data are nowavailable. Take one of the most popular photo sharing services Flickr
1
as example, it has accumulated several billions of images. Another ex-ample is Youtube
2, which is a video sharing Web site that is hosting
billions of videos. As the largest photo sharing site, Facebook3currently
stores hundreds of hundreds of billions of photos.
On the other hand, a multimedia entity does not appear in isola-
tion but is accompanied by various forms of textual metadata. One ofthe most typical examples is the surrounding text appearing around theembedded images or videos in the Web page (See Figure 11.1). Withrecent proliferation of social media sharing services, the newly emerg-
ing textual meatadata include user tags, ratings, comments, as well as
1http://www.ï¬‚ickr.com/
2http://www.youtube.com/
3http://www.facebook.com/Text Mining in Multimedia 363
hÆ‰Å¯Å½Ä‚ÄšÄÆŒ
'ÆŒÅ½ÆµÆ‰Æ
dÄ‚ÅÆ
^ÆµÆŒÆŒÅ½ÆµÅ¶ÄšÅÅ¶ÅdÄÇ†Æš
Å½ÅµÅµÄÅ¶ÆšÆ
Figure 11.2. Illustration of textual metadata of an image on a photo sharing Web
site.
the information about the uploaders and their social network (See Fig-
ure 11.2). These metadata, in particular the tags, have been found to be
an important resource for facilitating multimedia information process-ing and management. Given the wealth of research eï¬€orts that has beendone, there have been various studies in multimedia community on themining of textual metadata. In this chapter, a multimedia entity refersto an image or a video. For the sake of simplicity and without lost of
generality, we use the term image to refer to multimedia entity for the
rest of this chapter.
In this chapter, we ï¬rst review the related works on mining surround-
ing text for image retrieval as well as the recent research eï¬€orts thatexplore surrounding text for image annotation and clustering in Sec-tion 2. In Section 3, we provide a literature review on tag mining andshow that the main focus of existing tag mining works includes three as-
pects: tag ranking, tag reï¬nement, and tag information enrichment. In364 MINING TEXT DATA

Figure 11.3. A taxonomy consisting of the research works reviewed in this chapter.
Section 4, we survey the recent progress in integrating textual metadata
and visual content. We categorize the exiting works into two categories:the fusion of text and visual content as well as visual re-ranking. InSection 5, we provide a detailed discussion on recent research on crosstext and visual content mining. We organize all the works reviewed inthis chapter into a taxonomy as shown in Figure 11.3. The taxonomy
provides an overview of state-of-the-art research and helps us to identify
open research issues to be presented in Section 6.
2. Surrounding Text Mining
In order to enhance the content quality and improve user experience,
many hosting Web pages include diï¬€erent kinds of multimedia entities,like image or video. These multimedia entities are frequently embeddedas part of the text descriptions which we called the surrounding text.While there is no standard deï¬nition, surrounding text generally refers
to the text consisting of words, phrases or sentences that surrounds or
close to the embedded images, such as those that appear at the top,below, left or right region of images or connected via Web links. Theeï¬€ective use of surrounding texts is becoming increasingly importantfor multimedia retrieval. However, developing eï¬€ective extraction algo-rithm for the comprehensive analysis of surrounding text has been a verychallenging task. In many cases, automatically determining which page
region is more relevant to the image than the others could be diï¬ƒcult.
Moreover, how large the region nearby should be considered is still anopen question. Further, the quality of surrounding texts could be lowand inconsistent. These problems make it very hard to directly applythe surrounding text information to facilitate accurate retrieval. Thus,reï¬nement process or combining it with other cues is essential.
The earliest eï¬€orts on modeling and analyzing surrounding texts to
facilitate multimedia retrieval occurred in the 1990s. AltaVistaâ€™s A/VText Mining in Multimedia 365
Photo Finder applies textual and visual cues to index image collec-
tions [1]. The indexing terms are precomputed based on the HTMLdocuments containing the Web images. With a similar approach, the
WebSeer system harvests the information for indexing Web images from
two diï¬€erent sources: the related HTML text and the embedded im-age itself [12]. It extracts keywords from page title, ï¬le name, caption,alternative text, image hyperlinks, and body text titles. A weight iscalculated for each keyword based on its location inside a page. InPICITION system [40], an interesting approach is developed to exploitboth textual and visual information to index a pictorial database. Image
captions are used as an important cue to identify faces appearing in a
related newspaper photograph. The empirical study based on a data setcontaining 50 pictures and captions obtained from the Buffalo News
and theNew York Times is used to demonstrate the eï¬€ectiveness of the
PICITION system. While the system can be successfully adopted for ac-cessing photographs in newspaper or magazine, it is not straightforwardto apply it for Web image retrieval.
In [39], Smith and Chang proposed the WebSeek framework designed
to search images from the Web. The key idea is to analyze and classifythe Web multimedia objects into a predeï¬ned taxonomy of categories.Thus, an initial search can be performed to explore a catalog associatedwith the query terms. The image attribute (e.g., color histogram forimages) is then computed for similarity matching within the category.
Besides its eï¬ƒcacy in image retrieval, surrounding text has been ex-
plored for image annotation recently. Feng et al. presented a boot-
strapping framework to label and search Web images based on a setof predeï¬ned semantic concepts [9]. To achieve better annotation ef-fectiveness, a co-training scheme is designed to explore the associationbetween the text features computed using corresponding HTML docu-ments and visual features extracted from image content. Observing thatthe links between the visual content and the surrounding texts can be
modeled via Web page analysis, a novel method called Iterative Simi-
larity Propagation is proposed to reï¬ne the closeness between the Webimages and their annotations [50]. On the other hand, it is not hard toï¬nd that images from the same cluster may share many similar char-acteristics or patterns with respect to relevance to information needs.Consequently, accurate clustering is a very crucial technique to facili-tate Web multimedia search and many algorithms have recently been
proposed based on the analysis of surrounding texts and low level visual
features [3][13][34]. For example, Cai et al. [3] proposed a hierarchicalclustering method that exploits visual, textual, and link analysis. Awebpage is partitioned into blocks, and the textual and link information366 MINING TEXT DATA
of an image are extracted from the block containing that image. By us-
ing block-level link analysis techniques, an image graph is constructed.They then applied spectral techniques to ï¬nd a Euclidean embedding of
the images. As a result, each image has three types of representations:
visual feature, textual feature, and graph-based representation. Spectralclustering techniques are employed to cluster search results into variousclusters. Gao et al. [13] and Rege et al. [34] used a tripartite graph tomodel the relations among visual features, images and their surroundingtext. The clustering is performed by partitioning this tripartite graph.
3. Tag Mining
In newly emerging social media sharing services, such as the Flickr
and Youtube, users are encouraged to share multimedia data on the
Web and annotate content with tags. Here a tag is referred to as adescriptive keyword that describes the multimedia content at semanticor syntactic level. These tags have been found to be an important re-source for multimedia management and have triggered many innovativeresearch topics [61][51][38][36]. For example, with accurate tags, the re-trieval of multimedia content can be easily accomplished. The tags can
be used to index multimedia data and support eï¬ƒcient tag-based search.
Nowadays, many online media repositories, such as Flickr and Youtube,support tag-based multimedia search. However, since the tags are pro-vided by grassroots Internet users, they are often noisy and incompleteand there is still a gap between these tags and the actual content ofthe images[20][26][48]. This deï¬ciency has limited the eï¬€ectiveness oftag-based applications.
Recently, a wealth of research has been proposed to enhance the qual-
ity of human-provided tags. The existing works mainly focus on the fol-lowingthreeaspects: (a)tagranking, whichaimstodiï¬€erentiatethetagsassociated with the images with various levels of relevance; (b) tag re-ï¬nement with the purpose to reï¬ne the unreliable human-provided tags;and (c) tag information enrichment, which aims to supplement tags withadditional information [26]. In this section, we present a comprehensive
review of existing tag ranking, tag reï¬nement, and tag information en-
richment methods.
3.1 Tag Ranking
As shown in [25], the relevance level of the tags cannot be distin-
guished from the tag list of an image. The lack of relevance informationin the tag list has limited the application of tags. Recently, tag rankinghas been studied to infer the relevance levels of tags associated with anText Mining in Multimedia 367
Â…ÂƒÂ–Â’Â—Â•Â•Â›ÂÂ‹Â–Â–Â›ÂƒÂÂ‹ÂÂƒÂ
ÂŠÂƒÂ–Â„Â”Â‘Â™Â
Â”Â‹Â’Â‰ÂƒÂ”ÂƒÂ—Â–Â‘ÂÂ‘Â„Â‹ÂÂ‡Â—Â•Â–Â”ÂƒÂÂ‹Âƒ
ÂƒÂ—Â–Â‘Â˜Â‡ÂŠÂ‹Â…ÂÂ‡Â‰Â”ÂƒÂ•Â•Â’ÂÂƒÂÂ–Â•ÂÂ›Â…ÂƒÂ–Â†Â‡ÂÂ‡Â–Â‡ÂÂ‡Â„Â‡Â•Â–
Â„ÂƒÂ„Â›Â–Â‘Â’Â•Â‘Â•
Â–Â”Â‹Â’ÂƒÂ—Â–Â‘Â—Â•Â–Â”ÂƒÂÂ‹ÂƒÂÂ‡
Â†Â”Â‹Â˜Â‹ÂÂ‰Â•ÂÂ›
Figure 11.4. Examples of of tag reï¬nement. The left side of the ï¬gure shows the
original tags while the right side shows the reï¬ned tags. The technique is able toremoveirrelevanttagsandaddrelevanttagstoobtainbetterdescriptionofmultimediacontents.
image. As a pioneering work, Liu et al. [25] proposed to estimate tag
relevance scores using kernel density estimation, and then employ ran-dom walk to boost this primary estimation. Li et al. [22] proposed adata driven method for tag ranking. They learned the relevance scores
of tags by a neighborhood voting approach. Given an image and one
of its associated tag, the relevance score is learned by accumulating thevotes from the visual neighbors of the image. They then extended thework to multiple visual spaces [23]. They learned the relevance scoresof tags and ranked them by neighborhood voting in diï¬€erent featurespaces, and the results are aggregated with a score fusion or rank fusionmethod. Diï¬€erent aggregation methods have been investigated, such as
the average score fusion, Borda count and RankBoost. The results show
that a simple average fusion of scores is already able to perform closedto supervised fusion methods like RankBoost.
3.2 Tag Reï¬nement
User-provided tags are often noisy and incomplete. The study in [20]
shows that when a tag appears in a Flickr image, there is only about a50% chance that the tag is really relevant, and the study in [38] showsthat more than half of Flickr images are associated with less than three
tags. Tagreï¬nementtechnologiesareproposedaimingatobtainingmore368 MINING TEXT DATA
accurate and complete tags for multimedia description, as shown in Fig-
ure 11.4.
A lot of tag reï¬nement approaches have been developed based on
various statistical learning techniques. Most of them are based on the
following three assumptions.
The reï¬ned tags should not change too much from those providedby the users. This assumption is usually used to regularize the tagreï¬nement.
The tags of visually similar images should be closely related. Thisis a natural assumption that most automatic tagging methods arealso built upon.
Semantically close or correlative tags should appear with high cor-relation. For example, when a tag â€œseaâ€ exists for an image, thetags â€œbeachâ€ and â€œwaterâ€ should be assigned with higher conï¬-dence while the tag â€œstreetâ€ should have low conï¬dence.
For example, Chen et al. [6] ï¬rst trained a SVM classiï¬er for each tag
withthelooselylabeledpositiveandnegativesamples. Theclassiï¬ersareused to estimate the initial relevance scores of tags. They then reï¬ned
the scores with a graph-based method that simultaneously considers the
similarity between images and semantic correlation among tags. Xuet al. [52] proposed a tag reï¬nement algorithm from topic modelingpoint of view. A new graphical model named regularized latent Dirichletallocation (rLDA) is presented to jointly model the tag similarity andtag relevance. Zhu et al. [64] proposed a matrix decomposition method.They used a matrix to represent the image-tag relationship: the ( i,j)-
th element is 1 if the i-th image is associated with the j-th tag, and 0
otherwise. The matrix is then decomposed into a reï¬ned matrix plus anerrormatrix. Theyenforcedtheerrormatrixtobesparseandthereï¬nedmatrix to follow three principles: (a) let the matrix be low-rank; (b) iftwo images are visually similar, the corresponding rows are with highcorrelation; and (c) if two tags are semantically close, the correspondingvectors are with high correlation. Fan et al. [8] grouped images with
a target tag into clusters. Each cluster is regarded as a unit. The
initial relevance scores of the clusters are estimated and then reï¬ned bya random walk process. Liu et al. [24] adopted a three-step approach.The ï¬rst step ï¬lters out tags that are intrinsically content-unrelatedbased on the ontology in WordNet. The second step reï¬nes the tagsbased on the consistency of visual similarity and semantic similarity ofimages. The last step performs tag enrichment, which expands the tags
with their appropriate synonyms and hypericum.Text Mining in Multimedia 369
DQLPDOFDW WLJHU
WLJHUWLJHU
/RFDWLRQ
&RORU6KDSH7H[WXUH6L]H'RPLQDQFH6DOLHQF\ZÄÅÅÅ½Å¶Å¶Ä‚Å¯Ç‡ÆÅÆ
Í¾ÄÍ¿
ZDWHUELUGURDG
ELUG
URDGZDWHU
Í¾Ä‚Í¿
Figure 11.5. (a) An example of tag localization, which ï¬nds the regions that the
tags describe. (b) An illustration of tag information enrichment. It ï¬rst ï¬nds the
corresponding region of the target tag and then analyze the properties of the region.
3.3 Tag Information Enrichment
In the manual tagging process, generally human labelers will only
assign appropriate tags to multimedia entities without any additional
information, such as the image regions depicted by the correspondingtags. But by employing computer vision and machine learning tech-
nologies, certain information of the tags, such as the descriptive regions
and saliency, can be automatically obtained. We refer to these as taginformation enrichment.
Mostexistingworksemploythefollowingtwostepsfortaginformation
enrichment. First, tags are localized into regions of images or sub-clipsof videos. Second, the characteristics of the regions or sub-clips areanalyzed, and the information about the tags is enriched accordingly.
Figure 11.5 (a) illustrates the examples of tag localization for image
and video data. Liu et al. [28] proposed a method to locate image tagsto corresponding regions. They ï¬rst performed over-segmentation todecompose each image into patches and then discovered the relationshipbetween patches and tags via sparse coding. The over-segmented regionsare then merged to accomplish the tag-to-region process. Liu et al.extended the approach based on image search [29]. For a tag of the
target image, they collected a set of images by using the tag as query370 MINING TEXT DATA
withanimagesearchengine. Theythenlearnedtherelationshipbetween
the tag and the patches in this image set. The selected patches areused to reconstruct each candidate region, and the candidate regions are
ranked based on the reconstruction error. Liu et al. [27] accomplished
the tag-to-region task by regarding an image as a bag of regions andthen performed tag propagation on a graph, in which vertices are imagesand edges are constructed based on the visual link of regions. Feng etal. [10] proposed a tag saliency learning scheme, which is able to ranktags according to their saliency levels to an imageâ€™s content. They ï¬rstlocated tags to imagesâ€™ regions with a multi-instance learning approach.
In multi-instance learning, an image is regarded as a bag of multiple
instances, i.e., regions [58]. They then analyzed the saliency values ofthese regions. It can provide more comprehensive information whenan image is relevant to multiple tags, such as those describing diï¬€erentobjects in the image. Yang et al. [55] proposed a method to associatea tag with a set of properties, including location, color, texture, shape,size and dominance. They employed a multi-instance learning method
to establish the region that each tag is corresponding to, and the region
is then analyzed to establish the properties, as shown in Figure 11.5 (b).Sun and Bhowmick [41] deï¬ned a tagâ€™s visual representativeness basedon a large image set and the subset that is associated with the tag. Theyemployed two distance metrics, cohesion and separation, to estimate thevisual representativeness measure.
Ulges et al. [43] proposed an approach to localize video-level tags to
keyframes. Given a tag, it regards whether a keyframe is relevant as alatentrandomvariable. AnEM-styleprocessisthenadoptedtoestimatethe variables. Li et al. [21] employed a multi-instance learning approachto accomplish the video tag localization, in which video and shot areregarded as bag and shot, respectively.
By supplementing tags with additional information, a lot of tag-based
applications can be facilitated, such as tag-based image/video retrieval
and intelligent video browsing etc.
4. Joint Text and Visual Content Mining
Beyond mining pure textual metadata, researchers in multimedia
community have started making progress in integrating text and con-
tent for multimedia retrieval via joint text and content mining. The in-tegration of text and visual content has been found to be more eï¬€ectivethan exploiting purely text or visual content separately. The joint textand content mining in multimedia retrieval often comes down to ï¬nding
eï¬€ective mechanisms for fusing multi-modality information from textualText Mining in Multimedia 371
metadata and visual content. Existing research eï¬€orts can generally be
categorized into four paradigms: (a) linear fusion; (b) latent-space-basedfusion; (c) graph-based fusion; and (d) visual re-ranking that exploits
visual information to reï¬ne text-based retrieval results. In this section,
we ï¬rst brieï¬‚y review linear, latent space based, and graph based fusionmethods and then provide comprehensive literature review on visual re-ranking technology.
Linear fusion combines the retrieval results from various modalities
linearly [18][4][31]. In [18], visual content and text are combined in bothonline learning stage with relevance feedback and oï¬„ine keyword propa-
gation. In[31],linear,max,andaveragefusionstrategiesareemployedto
aggregate the search results from visual and textual modalities. Changet al. [4] adopted a query-class-dependent fusion approach. The criti-cal task in linear fusion is the estimation of fusion weights of diï¬€erentmodalities. A certain amount of training data is usually required forestimating these weights. The latent space based fusion assumes thatthere is a latent space shared by diï¬€erent modalities and thus unify dif-
ferent modalities by transferring the features of these modalities into the
shared latent space [63][62]. For example, Zhao et al. [63] adopted theLatent Semantic Indexing (LSI) method to fuse text and visual content.Zhang et al. [62] proposed a probabilistic context model to explicitlyexploit the synergy between text and visual content. The synergy is rep-resented as a hidden layer between the image and text modalities. Thishidden layer constitutes the semantic concepts to be annotated through
a probabilistic framework. An Expectation-Maximization (EM) based
iterative learning procedure is developed to determine the conditionalprobabilities of the visual features and the words given a hidden conceptclass. Latent space based methods usually require a large amount oftraining samples for learning the feature mapping from each modalityinto the uniï¬ed latent space. Graph based approach [49] ï¬rst builds therelations between diï¬€erent modalities, such as relations between images
and text using the Web page structure. The relations are then utilized to
iteratively update the similarity graphs computed from diï¬€erent modal-ities. The diï¬ƒculty of creating similarity graphs for billions of imageson the Web makes this approach insuï¬ƒciently scalable.
4.1 Visual Re-ranking
Visual re-ranking is emerging as one of the promising technique for
automated boosting of retrieval precision [42] [30] [55]. The basic func-tionality is to reorder the retrieved multimedia entities to achieve the
optimal rank list by exploiting visual content in a second step. In par-372 MINING TEXT DATA
ticular, given a textual query, an initial list of multimedia entities is
returned using the text-based retrieval scheme. Subsequently, the mostrelevant results are moved to the top of the result list while the less rel-
evant ones are reordered to the lower ranks. As such, the overall search
precision at the top ranks can be enhanced dramatically. According tothe statistical analysis model used, the existing re-ranking approachescan roughly be categorized into three categories including the clusteringbased, classiï¬cation based and graph based methods.
Cluster analysis is very useful to estimate the inter-entity similarity.
The clustering based re-ranking methods stem from the key observation
that a lot of visual characteristics can be shared by relevant images or
video clips. With intelligent clustering algorithms (e.g., mean-shift, K-means, and K-medoids), initial search results from text-based retrievalcan be grouped by visual closeness. One good example of clusteringbased re-ranking algorithms is an Information Bottle based scheme de-veloped by Hsu et al. [16]. Its main objective is to identify optimalclusters of images that can minimize the loss of mutual information.
The cluster number is manually conï¬gurated to ensure the each clus-
ter contains the same number of multimedia entities (about 25). Thismethod was evaluated using the TRECVID 2003-2005 data and signif-icant improvements were observed in terms of MAP measures. In [19],a fast and accurate scheme is proposed for grouping Web image searchresults into semantic clusters. For a given query, a few related semanticclusters are identiï¬ed in the ï¬rst step. Then, the cluster names relating
toqueryarederivedandusedastextkeywordsforqueryingimagesearch
engine. The empirical results from a set of user studies demonstrate animprovement in performance over Google image search results. It is nothard to show that the clustering based re-ranking methods can work wellwhen the initial search results contain many near-duplicate media docu-ments. However, for queries that return highly diverse results or withoutclear visual patterns, the performance of the clustering-based methods
is not guaranteed. Furthermore, the number of clusters has large impact
on the ï¬nal eï¬€ectiveness of the algorithms. However, determining theoptimal cluster number automatically is still an open research problem.
In the classiï¬cation based methods, visual re-ranking is formulated as
a binary classiï¬cation problem aiming to identify whether each searchresult is relevant or not. The major process for result list reorderingconsists of three major steps: (a) the selection of pseudo-positive and
pseudo-negative samples; (b) use the samples obtained in step (a) to
train a classiï¬cation scheme; and (c) reorder the samples according totheir relevance scores given by the trained classiï¬er. For existing classiï¬-cationmethods, pseudorelevancefeedback(PRF)isappliedtoselecttheText Mining in Multimedia 373
training examples. It assumes that: (a) a limited number of top-ranked
entities in the initial retrieval results are highly relevant to the searchqueries; and (b) automatic local analysis over the entities can be veryhelpful to reï¬ne query representation. In [54], the query images or videoclip examples are used as the pseudo-positive samples. The pseudo-negative samples are selected from either the least relevant samples in
the initial result list or the databases that contain less samples related
to the query. The second step of the classiï¬cation based methods aim totrain classiï¬ers and a wide range of statistical classiï¬ers can be adopted.They include the Support Vector Machine (SVM) [54], Boosting [53] andListNet [57]. The main weakness for the classiï¬cation based methods isthat the number and quality of training data required play a very im-portant role in constructing eï¬€ective classiï¬ers. However, in many real
scenarios, the training examples obtained via PRF are very noisy and
might not be adequate for training eï¬€ective classiï¬er. To address thisissue, Fergus et al. [11] used RANSAC to sample a training subset witha high percentage of relevant images. A generative constellation modelis learned for the query category while a background model is learnedfrom the query â€œthingsâ€. Images are re-ranked based on their likeli-hood ratio. Observing that discriminative learning can lead to superior
results, Schroï¬€ et al. [35] ï¬rst learned a query independent text based
re-ranker. The top ranked results from the text based re-ranking arethen selected as positive training examples. Negative training examplesare picked randomly from the other queries. A binary SVM classiï¬eris then used to re-rank the results on the basis of visual features. Thisclassiï¬er is found to be robust to label noise in the positive training setas long as the non-relevant images are not visually consistent. Better
training data can be obtained from online knowledge resources if the set
of queries restricted. For instance, Wang et al. [44] learned a generativetext model from the queryâ€™s Wikipedia
4page and a discriminative im-
age model from the Caltech [15] and Flickr data sets. Search results arethen re-ranked on the basis of these learned probability models. Someuser interactions are required to disambiguate the query.
Graphs provide a natural and comprehensive way to explore complex
relations between data at diï¬€erent levels and have been applied to awide range of applications [59][46][47][60]. With the graph based re-ranking methods, the multimedia entities in top ranks and their associa-tions/dependencies can be represented as a collection of nodes (vertices)and edges. The local patterns or salient features discover using graph
4http://www.wikipedia.org/374 MINING TEXT DATA
analysis are very helpful to improve eï¬€ectiveness of rank lists. In [16],
Hsuetal. modeledthere-rankingprocessasarandomwalkoverthecon-text graph. In order to eï¬€ectivelyleveragethe retrievedresults fromtextsearch, each sample corresponds to a â€œdongleâ€ node containing rankingscore based on text. For the framework, edges between â€œdongleâ€ nodesare weighted with multi-modal similarities. In many cases, the struc-
ture of large scale graphs can be very complex and this easily makes
related analysis process very expensive in terms of computational cost.Thus, Jing and Baluja proposed a VisualRank framework to eï¬ƒcientlymodel similarity of Google image search results with graph [17]. Theframework casts the re-ranking problem as random walk on an aï¬ƒnitygraph and reorders images according to the visual similarities. The ï¬-nal result list is generated via sorting the images based on graph nodesâ€™
weights. In[42], Tianetal., presentedaBayesianvideosearchre-ranking
framework formulating the re-ranking process as an energy minimizationproblem. The main design goal is to optimize the consistency of rank-ing scores over visually similar videos and minimize the disagreementbetween the optimal list and the initial list. The method achieves aconsistently better performance over several earlier proposed schemeson the TRECVID 2006 and 2007 data sets. The graph based re-ranking
algorithms mentioned above generally do not consider any initial super-
vision information. Thus, the performance is signiï¬cantly dependenton the statistical properties of top ranked search results. Motivated bythis observation, Wang et al, proposed a semi-supervised framework toreï¬ne the text based image retrieval results via leveraging the data dis-tribution and the partial supervision information obtained from the topranked images [45]. Indeed, graph analysis has been shown to be a very
powerful tool for analyzing and identifying salient structure and useful
patterns inside the visual search results. With recent progresses in graphmining, this research stream is expected to continue to make importantcontributions to improve visual re-ranking from diï¬€erent perspectives.
5. Cross Text and Visual Content Mining
Although the joint text and visual content mining approaches de-
scribed above facilitate image retrieval, they require that the test imageshave associated text modality. However, in some real world applications,
images may not always have associated text. For example, most surveil-
lance images/videos in in-house repository are not accompanied withany text. Even on social media Website such as the Flickr, there exista substantial number of images without any tags. In such cases, jointText Mining in Multimedia 375

Figure 11.6. An illustration of diï¬€erent types of learning paradigms using image
classiï¬cation/clustering in the domains of apple and banana. Adapted from [56].
text and visual content mining cannot be applied due to missing text
modality.
Recently, cross text and visual content mining has been studied in the
context of transfer learning techniques. This class of techniques empha-
sizes the transferring of knowledge across diï¬€erent domains or tasks [32].
Cross text and visual content mining does not require that a test imagehasanassociatedtextmodality, andisthusbeneï¬cialtodealingwiththeimages without any text by propagating the semantic knowledge fromtext to images
5. It is also motivated by two observations. First, visual
contentofimagesismuchmorecomplicatedthanthetextfeature. Whilethetextualwordsareeasiertointerpret, thereexistatremendousseman-
tic gap between visual content and high-level semantics. Second, image
understanding becomes particularly challenging when only a few labeledimages are available for training. This is a common challenge, since itis expensive and time-consuming to obtain labeled images. On the con-trary, labeled/unlabeled text data are relatively easier to collect. Forexample, millions of categorized text articles are freely available in Web
5Cross text and visual content can also facilitate text understanding in special cases by
propagating knowledge from images to text.376 MINING TEXT DATA
text collections, such as Wikipedia, covering a wide range of topics from
culture and arts, geography and places, history and events, to naturaland physical science. A large number of Wikipedia articles are indexed
by thousands of categories in these topics [33]. This provides abundant
labeled text data. Thus, it is desirable to propagate semantic knowledgefrom text to images to facilitate image understanding. However, it isnot trivial to transfer knowledge between various domains/tasks due tothe following challenges:
The target data may be drawn from a distribution diï¬€erent fromthe source data.
The target and source data may be in diï¬€erent feature spaces (e.g.,
image and text) and there may be no correspondence between
instances in these spaces.
The target and source tasks may have diï¬€erent output spaces.
While the traditional transfer learning techniques focus on the dis-
tribution variance problem, the recent proposed heterogenous transferlearning approaches aim to tackle both the distribution variance andheterogenous feature space problems [56][7][65][33], or all the three chal-
lenges listed above [37]. Figure 11.6 from [56] presents an intuitive illus-
tration of four learning paradigms, including traditional machine learn-
ing, transfer learning across diï¬€erent distributions, multi-view learningand heterogenous transfer learning. As we can see, heterogenous trans-fer learning is usually much more challenging due to the unknown cor-respondence across the distinct feature spaces. In order to learn theunderlying correspondence for knowledge transformation, a â€œsemantic
bridgeâ€ is required. The â€œsemantic bridgeâ€ can be obtained from the
co-occurrence information between text and images or the linkage in-formation in social media networks. For example, while the traditionalwebpages provide the co-occurrence information between text and im-ages, the social media sites contain a large number of linked informationbetween diï¬€erent types of entities, such as the text articles, tags, posts,imagesandvideos. Thislinkageinformationprovideaâ€œsemanticbridgeâ€
to learn the underlying correspondence [2].
Most existing works exploit the tag information that provide text-to-
image linking information. As a pioneering work, Dai et al. [7] showed
that such information can be eï¬€ectively leveraged for transferring knowl-edge between text and images. The key idea of [7] is to construct acorrespondence between the images and the auxiliary text data withthe use of tags. Probabilistic latent semantic analysis (PLSA) model
is employed to construct a latent semantic space which can be used forText Mining in Multimedia 377
transferring knowledge. Chen et al. [56] proposed the concept of hetero-
geneous transfer learning and applied it to improve image clustering byleveraging auxiliary text data. They collected annotated images fromthe social web, and used them to construct a text to image mapping.The algorithm is referred to as aPLSA (Annotated Probabilistic La-tent Semantic Analysis). The key idea is to unify two diï¬€erent kinds
of latent semantic analysis in order to create a bridge between the text
and images. The ï¬rst kind of technique performs PLSA analysis on thetarget images, which are converted to an image instance-to-feature co-occurrence matrix. The second kind of PLSA is applied to the annotatedimage data from social Web, which is converted into a text-to-image fea-ture co-occurrence matrix. In order to unify those two separate PLSAmodels, these two steps are done simultaneously with common latent
variables used as a bridge linking them. It has been shown in [5] that
such a bridging approach leads to much better clustering results. Zhuet al. [65] discussed how to create the connections between images andtext with the use of tag data. They showed how such links can be usedmore eï¬€ectively for image classiï¬cation. An advantage of [65] is that itexploits unlabeled text data instead of labeled text as in [7].
In contrast to these methods that exploit tag information to link im-
ages and auxiliary text articles, Qi et al. [33] proposed to learn a â€œtrans-
latorâ€ which can directly establish the semantic correspondence betweentextandimageseveniftheyarenewinstancesoftheimagedatawithun-known correspondence to the text articles. This capability increase theï¬‚exibility of the approach and makes it more widely applicable. Speciï¬-cally, theycreatedanewtopicspaceintowhichboththetextandimagesare mapped. A translator is then learned to link the instances across
heterogeneous text and image spaces. With the resultant translator,
the semantic labels can be propagated from any labeled text corpus toany new image by a process of cross-domain label propagation. Theyshowed that the learned translator can eï¬€ectively convert the semanticsfrom text to images.
6. Summary and Open Issues
Inthischapter, wehavereviewedtheactiveresearchontextminingin
multimedia community, including surrounding text mining, tag mining,
joint text and visual content mining, and cross text and visual content
mining. Although research eï¬€orts in this ï¬led have made great progressin various aspects, there are still many open research issues that need tobe explored. Some examples are listed and discussed as follows.378 MINING TEXT DATA
Joint text and visual content multimedia ranking
Despite the success of visual re-ranking in multimedia retrieval, visual
re-ranking only employs the visual content to reï¬ne text-based retrieval
results; visual content has not been used to assist in learning the rank-ing model of search engine, and sometimes it is only able to bring inlimited performance improvements. In particular, if text-based ranking
model is biased or over-ï¬tted, re-ranking step will suï¬€er from the error
that is propagated from the initial results, and thus the performanceimprovement will be negatively impacted. Therefore, it is worthwhileto simultaneously exploit textual metadata and visual content to learna uniï¬ed ranking model. A preliminary work has been done in [14],where a content-aware ranking model is developed to incorporate visualcontent into text-based ranking model learning. It shows that the in-
corporation of visual content into ranking model learning can result in a
more robust and accurate ranking model since noise in textual featurescan be suppressed by visual information.
Scalable text mining for large-scale multimedia man-
agement
Despite of the success of existing text mining in multimedia, most
existing techniques suï¬€er from diï¬ƒculties in handling large-scale multi-
media data. Huge amount of training data or high computation powersare usually required by existing methods to achieve acceptable perfor-mance. However, it is too diï¬ƒcult, or even impossible, to meet this
requirement in real-world applications. Thus there is a compelling need
to develop scalable text mining techniques to facilitate large-scale mul-timedia management.
Multimedia social network mining
In recent years, we have witnessed the emergence of multimedia social
network communities like Napster6, Facebook7, and Youtube, where
millions of users and billions of multimedia entities form a large-scalemultimedia social network. Multimedia social networking is becoming
an important part of media consumption for Internet users. It brings
in new and rich metadata, such as user preferences, interests, behaviors,social relationships, and social network structure etc. These informa-tion present new potential for advancing current multimedia analysis
6http://music.napster.com/
7http://www.facebook.com/Text Mining in Multimedia 379
techniques and also trigger diverse multimedia applications. Numerous
researchtopicscanbeexplored, including(a)thecombinationofconven-tional techniques with information derived from social network commu-
nities; (b) fusion analysis of content, text, and social network data; and
(c) personalized multimedia analysis in social networking environments.
Acknowledgements
This work was in part supported by A*Star Research Grant R-252-
000-437-305 and NRF (National Research Foundation of Singapore) Re-search Grant R-252-300-001-490 under the NExT Center.
References
[1] Altavistaâ€™s a/v photo ï¬nder. http://www.altavista.com/sites/
search/simage.
[2] C. C. Aggarwal, H. Wang. Text Mining in Social Networks.S o c i a l
Network Data Analytics, Springer, 2011.
[3] D. Cai, X. He, Z. Li, W.-Y. Ma, and J.-R. Wen. Hierarchical clus-
tering of www image search results using visual, textual and linkinformation. In Proceedings of the ACM Conference on Multimedia ,
2004.
[4] S.-F. Chang, W. Hsu, W. Jiang, L. Kennedy, D. Xu, A. Yanagawa,
and E. Zavesky. Columbia university trecvid-2006 video search andhigh-level feature extraction. In Proceedings of NIST TRECVID
workshop, 2006.
[5] L. Chen and A. Roy. Event detection from Flickr data through
wavelet-based spatial analysis. In Proceedings of the ACM confer-
ence on Information and knowledge management , pages 523â€“532.
ACM, 2009.
[6] L. Chen, D. Xu, I. W. Tsang, and J. Luo. Tag-based web photo
retrieval improved by batch mode re-tagging. In Proceedings of the
IEEE International Conference on Computer Vision and Pattern
Recognition , 2010.
[7] W. Dai, Y. Chen, G.-R. Xue, Q. Yang, and Y. Yu. Translated
learning:Transferlearningacrossdiï¬€erencefeaturespaces. In NIPS,
pages 353â€“360, 2008.
[8] J.Fan,Y.Shen,N.Zhou,andY.Gao. Harvestinglarge-scaleweakly-
tagged image databases from the web. In Proceedings of the IEEE
International Conference on Computer Vision and Pattern Recog-nition, 2010.380 MINING TEXT DATA
[9] H. Feng, R. Shi, and T.-S. Chua. A bootstrapping framework for
annotating and retrieving www images. In Proceedings of the ACM
Conference on Multimedia , 2004.
[10] S. Feng, C. Lang, and D. Xu. Beyond tag relevance: integrating
visual attention model and multi-instance learning for tag saliency
ranking. In Proceedings of I nternational Conference on Image and
Video Retrieval , 2010.
[11] R. Fergus, P. Perona, and A. Zisserman. A visual category ï¬lter
for google images. In Proceedings of the European Conference on
Computer Vision , 2004.
[12] C. Frankel, M. J. Swain, and V. Athitsos. Webseer: An image
search engine for the world wide web. Technical report, Univer-sity of Chicago, Computer Science Department, 1996.
[13] B. Gao, T.-Y. Liu, Q. Tao, X. Zheng, Q. Cheng, and W.-Y. Ma.
Web image clustering by consistent utilization of visual featuresand surrounding texts. In Proceedings of the ACM Conference on
Multimedia , 2005.
[14] B. Geng, L. Yang, C. Xu, and X.-S. Hua. Content-aware ranking
for visual search. In Proceedings of the I nternational Conference on
Computer Vision and Pattern R ecognition , 2010.
[15] G. Griï¬ƒn, A. Holub, and P. Perona. Caltech-256 object category
dataset. Technical Report 7694, California Institute of Technology,2007.
[16] W. Hsu, L. Kennedy, , and S.-F. Chang. Reranking methods for
visual search. IEEE Multimedia , 14:14â€“22, 2007.
[17] F. Jing and S. Baluja. Visualrank: Applying pagerank to large-scale
image search. IEEE Transactions on Pattern Analysis and Machine
Intelligence , 30:1877â€“1890, 2008.
[18] F. Jing, M. Li, H.-J. Zhang, and B. Zhang. A uniï¬ed framework for
image retrieval using keyword and visual features. IEEE Transac-
tions on Image Processing , 2005.
[19] F. Jing, C. Wang, Y. Yao, K. Deng, L. Zhang, and W.-Y. Ma.
Igroup: Web image search results clustering. In Proceedings of the
ACM Conference on Multimedia , pages 377â€“384, 2006.
[20] L. S. Kennedy, S. F. Chang, and I. V. Kozintsev. To search or to
label? predicting the performance of search-based automatic imageclassiï¬ers. In Proceedings of the ACM International Workshop on
Multimedia Information Retrieval , 2006.Text Mining in Multimedia 381
[21] G. Li, M. Wang, Y. T. Zheng, Z.-J. Zha, H. Li, and T.-S. Chua.
Shottagger: Tag location for internet videos. In Proceedings of the
ACM International Conference on Multimedia Retrieval , 2011.
[22] X. Li, C. G. Snoek, and M. Worring. Learning social tag relevance
by neighbor voting. Pattern R ecognition Letters , 11(7), 2009.
[23] X. Li, C. G. Snoek, and M. Worring. Unsupervised multi-feature
tag relevance learning for social image retrieval. In Proceedings of
the International Conference on Image and Video Retrieval , 2010.
[24] D. Liu, X. C. Hua, M. Wang, and H. Zhang. Image retagging. In
Proceedings of the ACM Conference on Multimedia , 2010.
[25] D.Liu,X.-S.Hua,L.Yang,M.Wang,andH.-J.Zhang. Tagranking.
InProceedings of the I nternational Conference on World Wide Web ,
2009.
[26] D. Liu, X.-S. Hua, and H.-J. Zhang. Content-based tag process-
ing for internet social images. Multimedia Tools and Application ,
51:723â€“738, 2010.
[27] D. Liu, S. Yan, Y. Rui, and H. J. Zhang. Uniï¬ed tag analysis
with multi-edge graph. In Proceedings of the ACM Conference on
Multimedia , 2010.
[28] X. Liu, B. Cheng, S. Yan, J. Tang, T. C. Chua, and H. Jin. Label
to region by bi-layer sparsify priors. In Proceedings of the ACM
Conference on Multimedia , 2009.
[29] X. Liu, S. Yan, J. Luo, J. Tang, Z. Huang, and H. Jin. Nonpara-
metric label-to-region by search. In Proceedings of the IEEE Inter-
national Conference on Computer Vision and Pattern R ecognition ,
2010.
[30] Y. Liu, T. Mei, and X.-S. Hua. Crowdreranking: Exploring multiple
search engines for visual search reranking. In Proceedings of the
ACM SIGIR Conference , 2009.
[31] T. Mei, Z.-J. Zha, Y. Liu, M. Wang, and et al. Msra at trecvid 2008:
High-level feature extraction and automatic search. In Proceedings
of NIST TRECVID workshop , 2008.
[32] S. J. Pan and Q. Yang. A survey on transfer learning. IEEE Trans-
actions on Knowledge and Data Engineering , 22(10), 2010.
[33] G.-J. Qi, C. C. Aggarwal, and T. Huang. Towards semantic knowl-
edge propagation from text corpus to web images. In Proceedings
of the International Conference on World Wide Web , 2011.
[34] M. Rege, M. Dong, and J. Hua. Graph theoretical framework for
simultaneously integrating visual and textual features for eï¬ƒcient382 MINING TEXT DATA
web image clustering. In Proceedings of the I nternational Confer-
ence on World Wide Web , 2008.
[35] F. Schroï¬€, A. Criminisi, and A. Zisserman. Harvesting images
databases from the web. In Proceedings of the I nternational Con-
ference on Computer Vision , 2007.
[36] D. A. Shamma, R. Shaw, P. L. Shafton, and Y. Liu. Watch what
i watch: using community activeity to understand content. In Pro-
ceedings of the ACM Workshop on Multimedia Information Re-
trieval, 2007.
[37] X. Shi, Q. Liu, W. Fan, P. S. Yu, and R. Zhu. Transfer learn-
ing on heterogenous feature spaces via spectral tranformation. InProceedings of the I nternational Conference on Data Mining , 2010.
[38] B.SigurbjÂ¨ ornssonandR.V.Zwol. Flickrtagrecommendationbased
on collective knowledge. In Proceedings of I nternational Conference
on World Wide Web , 2008.
[39] J. Smith and S.-F. Chang. Visually searching the web for content.
IEEE Multimedia , 4:12â€“20, 1995.
[40] R. Srihari. Automatic indexing and content-based retrieval of cap-
tioned images. IEEE Computer , 28:49â€“56, 1995.
[41] A. Sun and S. S. Bhowmick. Quantifying tag representativeness of
visual content of social images. In Proceedings of the ACM Confer-
ence on Multimedia , 2010.
[42] X. Tian, L. Yang, J. Wang, Y. Yang, X. Wu, and X.-S. Hua.
Bayesian video search reranking. In Proceedings of the ACM Con-
ference on Multimedia , 2008.
[43] A. Ulges, C. Schulze, D. Keysers, and T. M. Breuel. Identifying
relevant frames in weakly labeled videos for training concept detec-
tors. InProceedings of the International Conference on Image and
Video Retrieval , 2008.
[44] G. Wang and D. A. Forsyth. Object image retrieval by exploiting
online knowledge resources. In Proceedings of the IEEE Conference
on Computer Vision and Pattern R ecognition , 2008.
[45] J.Wang,Y.-G.Jiang,andS.-F.Chang. Labeldiagnosisthroughself
tuningforwebimagesearch. In Proceedings of the IEEE Conference
on Computer Vision and Pattern R ecognition .
[46] M. Wang, X. S. Hua, R. Hong, J. Tang, G. J. Qi, and Y. Song. Uni-
ï¬ed video annotation via multi-graph learning. IEEE Transactions
on Circuits and Systems for Video Technology , 19(5), 2009.Text Mining in Multimedia 383
[47] M. Wang, X. S. Hua, J. Tang, and R. Hong. Beyond distance mea-
surement: Constructing neighborhood similarity for video annota-tion.IEEE Transactions on Multimedia , 11(3), 2009.
[48] M. Wang, B. Ni, X.-S. Hua, and T.-S. Chua. Assistive multimedia
tagging:Asurveyofmultimediataggingwithhuman-computerjointexploration. ACM Computing Survey , 2011.
[49] X.-J. Wang, W.-Y. Ma, G.-R. Xue, and X. Li. Multi-model simi-
larity propagation and its application for web image retrieval. InProceedings of the ACM Conference on Multimedia , pages 944â€“951,
2004.
[50] X.-J. Wang, W.-Y. Ma, L. Zhang, and X. Li. Iteratively clustering
web images based on link and attribute reinforcements. In Proceed-
ings of the ACM Conference on Multimedia , 2005.
[51] L. Wu, X.-S. Hua, N. Yu, W.-Y. Ma, and S. Li. Flickr distance. In
Proceedings of the ACM Conference on Multimedia , 2008.
[52] H.Xu,J.Wang,X.-S.Hua,andS.Li. Tagreï¬nementbyregularized
LDA. In Proceedings of the ACM Conference on Multimedia , 2009.
[53] R. Yan and A. G. Hauptmann. Co-retrieval: A boosted reranking
approach for video retrieval. In Proceedings of the ACM Conference
on Image and Video Retrieval , 2004.
[54] R. Yan, A. G. Hauptmann, and R. Jin. Multimedia search with
pseudo-relevance feedback. In Proceedings of the ACM Conference
on Image and Video Retrieval , 2003.
[55] K. Yang, X.-S. Hua, M. Wang, and H. C. Zhang. Tagging tags. In
Proceedings of the ACM Conference on Multimedia , 2010.
[56] Q. Yang, Y. Chen, G.-R. Xue, W. Dai, and Y. Yu. Heterogeneous
transfer learning from image clustering via the social web. In Pro-
ceedings of the Joint Conference of the Annual Meeting of the ACL ,
2009.
[57] Y.-H. Yang, P. Wu, C. W. Lee, K. H. Lin, W. Hsu, and H. H. Chen.
Contextseer: Context search and recommendation at query time forshared consumer photos. In Proceedings of the ACM Conference on
Multimedia , 2008.
[58] Z.-J. Zha, X.-S. Hua, T. Mei, J. Wang, G.-J. Qi, and Z. Wang.
Joint multi-label multi-instance learning for image classiï¬cation.InProceedings of the IEEE Conference on Computer V ision and
Pattern Recognition , 2008.
[59] Z.-J. Zha, T. Mei, J. Wang, X.-S. Hua, and Z. Wang. Graph-based
semi-supervised learning with multiple labels. Journal of Visual
Communication and Image Representation , 2009.384 MINING TEXT DATA
[60] Z.-J. Zha, M. Wang, Y.-T. Zheng, Y. Yang, R. Hong, and T.-S.
Chua. Interactive video indexing with statistical active learning.IEEE Transactions on Multimedia , 2011.
[61] Z.-J. Zha, L. Yang, T. Mei, M. Wang, and Z. Wang. Viusal query
suggestion. In Proceedings of the ACM Conference on Multimedia ,
2009.
[62] R. Zhang, Z. M. Zhang, M. Li, W.-Y. Ma, and H.-J. Zhang. A
probabilistic semantic model for image annotation and multi-modal
image retrieval. In Proceedings of the I nternational Conference on
Computer Vision , pages 846â€“851, 2005.
[63] R. Zhao and W. I. Grosky. Narrowing the semantic gap - improved
text-based web document retireval using visual fetures. IEEE
Transactions on Multimedia , 4, 2002.
[64] G. Zhu, S. Yan, and Y. Ma. Image tag reï¬nement towards low-
rank, content-tag prior and error sparsity. In Proceedings of the
ACM Conference on Multimedia , 2010.
[65] Y. Zhu, Y. Chen, Z. Lu, S. J. Pan, G.-R. Xue, Y. Yu, and Q. Yang.
Heterogeneous transfer learning for image classiï¬cation. In Proceed-
ings of the AAAI Conference on Artiï¬cial Intelligence , 2011.Chapter 12
TEXT ANALYTICS IN SOCIAL MEDIA
Xia Hu
Computer Science and Engineering
Arizona State University
xiahu@asu.edu
Huan Liu
Computer Science and Engineering
Arizona State University
huanliu@asu.edu
Abstract The rapid growth of online social media in the form of collaboratively-
created content presents new opportunities and challenges to both pro-
ducers and consumers of information. With the large amount of data
produced by various social media services, text analytics provides an
eï¬€ective way to meet usresâ€™ diverse information needs. In this chapter,
we ï¬rst introduce the background of traditional text analytics and the
distinct aspects of textual data in social media. We next discuss the
research progress of applying text analytics in social media from diï¬€er-
ent perspectives, and show how to improve existing approaches to text
representation in social media, using real-world examples.
Keywords: Text Analytics, Social Media, Text Representation, Time Sensitivity,
Short Text, Event Detection, Collaborative Question Answering, Social
Tagging, Semantic Knowledge
1. Introduction
Social media such as blogs, microblogs, discussion forums and multi-
mediasharingsitesareincreasinglyusedforuserstocommunicatebreak-
ing news, participate in events, and connect to each other anytime, from
anywhere. The social media sites play a very important role in current
Â© Springer Science+Business Media, LLC 2012 385  C.C. Aggarwal and C.X. Zhai(eds.),Mining Text Data , DOI 10.1007/978-1-4614-3223-4_12,